{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSF Cough Audio Classifier with Deep Learning\n",
    "\n",
    "Build a CNN sound classifier (transfer learning from pretrained VGGish model) using melspectograms from OSF cough audio dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.utils import multi_gpu_model\n",
    "import numpy as np\n",
    "import json\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vggish_params import *\n",
    "from vggish import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import sys\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras import backend as K\n",
    "\n",
    "import vggish_params as params\n",
    "\n",
    "# weight path\n",
    "WEIGHTS_PATH = '/Users/xt/Desktop/OSF/vggish_audioset_weights_without_fc2.h5'\n",
    "WEIGHTS_PATH_TOP = '/Users/xt/Desktop/OSF/vggish_audioset_weights.h5'\n",
    "\n",
    "def VGGish(load_weights=True, weights='audioset',\n",
    "           input_tensor=None, input_shape=None,\n",
    "           out_dim=None, include_top=True, pooling='max'):\n",
    "    '''\n",
    "    An implementation of the VGGish architecture.\n",
    "\n",
    "    :param load_weights: if load weights\n",
    "    :param weights: loads weights pre-trained on a preliminary version of YouTube-8M.\n",
    "    :param input_tensor: input_layer\n",
    "    :param input_shape: input data shape\n",
    "    :param out_dim: output dimension\n",
    "    :param include_top:whether to include the 3 fully-connected layers at the top of the network.\n",
    "    :param pooling: pooling type over the non-top network, 'avg' or 'max'\n",
    "\n",
    "    :return: A Keras model instance.\n",
    "    '''\n",
    "\n",
    "    if weights not in {'audioset', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `audioset` '\n",
    "                         '(pre-training on audioset).')\n",
    "\n",
    "    if out_dim is None:\n",
    "        out_dim = params.EMBEDDING_SIZE\n",
    "\n",
    "    # input shape\n",
    "    if input_shape is None:\n",
    "        input_shape = (params.NUM_FRAMES, params.NUM_BANDS, 1)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        aud_input = Input(shape=input_shape, name='input_1')\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            aud_input = Input(tensor=input_tensor, shape=input_shape, name='input_1')\n",
    "        else:\n",
    "            aud_input = input_tensor\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv1')(aud_input)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool1')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool2')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv3/conv3_1')(x)\n",
    "    x = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv3/conv3_2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool3')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv4/conv4_1')(x)\n",
    "    x = Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv4/conv4_2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool4')(x)\n",
    "\n",
    "    if include_top:\n",
    "        # FC block\n",
    "        x = Flatten(name='flatten_')(x)\n",
    "        x = Dense(4096, activation='relu', name='vggish_fc1/fc1_1')(x)\n",
    "        x = Dense(4096, activation='relu', name='vggish_fc1/fc1_2')(x)\n",
    "        x = Dense(out_dim, activation='relu', name='vggish_fc2')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = aud_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='VGGish')\n",
    "\n",
    "    # load weights\n",
    "    if load_weights:\n",
    "        if weights == 'audioset':\n",
    "            if include_top:\n",
    "                model.load_weights(WEIGHTS_PATH_TOP)\n",
    "            else:\n",
    "                model.load_weights(WEIGHTS_PATH)\n",
    "        else:\n",
    "            print(\"failed to load weights\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 128, 128\n",
    "\n",
    "input_tensor = Input(shape=(128,128,1))\n",
    "\n",
    "nb_training_samples = 3716\n",
    "nb_validation_samples = 1219# Set parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure training and validation data generators\n",
    "\n",
    "Provide paths to training and testing set directores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3716 images belonging to 2 classes.\n",
      "Found 1219 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# training generator configuration\n",
    "training_data_dir = '/Users/xt/Desktop/OSF/melspectrograms/train/'\n",
    "\n",
    "\n",
    "#img = img.resize((w, h), Image.LANCZOS).convert('L')\n",
    "#image_np = np.array(img )\n",
    "#image_np = np.expand_dims(image_np, -1)\n",
    "\n",
    "\n",
    "training_datagen = image.ImageDataGenerator(\n",
    "    rescale=1./255) #, rotation_range = 90)\n",
    "\n",
    "training_generator = training_datagen.flow_from_directory(\n",
    "    training_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,color_mode=\"grayscale\", shuffle=True)\n",
    "\n",
    "# validation generator configuration\n",
    "validation_data_dir ='/Users/xt/Desktop/OSF/melspectrograms/validation/'\n",
    "\n",
    "validation_datagen = image.ImageDataGenerator(\n",
    "    rescale=1./255) #, rotation_range = 90)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size, color_mode=\"grayscale\", shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Model: \"VGGish\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 128, 128, 64)      640       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3/conv3_1 (Conv2D)       (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv3/conv3_2 (Conv2D)       (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv4/conv4_1 (Conv2D)       (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv4/conv4_2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 4,499,712\n",
      "Trainable params: 4,499,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = VGGish(include_top=False, load_weights=True)\n",
    "print('Model loaded.')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 164,482\n",
      "Trainable params: 164,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01)))\n",
    "top_model.add(Dropout(0.2))\n",
    "top_model.add(Dense(128, activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01)))\n",
    "top_model.add(Dropout(0.2))\n",
    "top_model.add(Dense(2, activation='softmax'))\n",
    "top_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine base model with top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_model.load_weights('bootlneck_fc_model.h5')\n",
    "model = Model(inputs=base_model.input, outputs=top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics, optimizers\n",
    "\n",
    "def top_1_accuracy(y_true, y_pred):\n",
    "    return metrics.top_k_categorical_accuracy(y_true, y_pred, k=1)\n",
    "\n",
    "#for layer in model.layers[:num_layers_to_freeze]:\n",
    "#    layer.trainable = False\n",
    "\n",
    "for layers in base_model.layers:\n",
    "    layers.trainable = False\n",
    "\n",
    "# use nesterov accelrated gradient descent ??\n",
    "# optimizer=optimizers.SGD(lr=1e-4, momentum=0.9, decay=1e-6, nesterov=True)\n",
    "\n",
    "#model.compile(optimizer=optimizers.SGD(lr=2e-5, momentum=0.9), \n",
    "#                      loss='categorical_crossentropy', \n",
    "#                      metrics=['accuracy', top_1_accuracy])\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(), \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=['accuracy', top_1_accuracy])\n",
    "\n",
    "# parallel_model.compile(optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), \n",
    "#                       loss='categorical_crossentropy', \n",
    "#                       metrics=['accuracy', top_1_accuracy])\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "#model_filename = \"vggish_model_{}_frozen_layers.json\".format(num_layers_to_freeze)\n",
    "model_filename = \"vggish_model_frozen_layers.json\"\n",
    "\n",
    "with open(model_filename, \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 128, 128, 64)      640       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3/conv3_1 (Conv2D)       (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv3/conv3_2 (Conv2D)       (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv4/conv4_1 (Conv2D)       (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv4/conv4_2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 2)                 164482    \n",
      "=================================================================\n",
      "Total params: 4,664,194\n",
      "Trainable params: 164,482\n",
      "Non-trainable params: 4,499,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for layers in base_model.layers:\n",
    "#    print(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-37bc4c1e38ad>:27: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.6592 - accuracy: 0.6103 - top_1_accuracy: 0.6103WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 105s 4s/step - loss: 0.6592 - accuracy: 0.6103 - top_1_accuracy: 0.6103 - val_loss: 0.6248 - val_accuracy: 0.6169 - val_top_1_accuracy: 0.6169\n",
      "Epoch 2/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.6069 - accuracy: 0.6531 - top_1_accuracy: 0.6531WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 103s 3s/step - loss: 0.6069 - accuracy: 0.6531 - top_1_accuracy: 0.6531 - val_loss: 0.5673 - val_accuracy: 0.7022 - val_top_1_accuracy: 0.7022\n",
      "Epoch 3/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5685 - accuracy: 0.6940 - top_1_accuracy: 0.6940WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 105s 3s/step - loss: 0.5685 - accuracy: 0.6940 - top_1_accuracy: 0.6940 - val_loss: 0.5566 - val_accuracy: 0.6924 - val_top_1_accuracy: 0.6924\n",
      "Epoch 4/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5701 - accuracy: 0.6967 - top_1_accuracy: 0.6967WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 106s 4s/step - loss: 0.5701 - accuracy: 0.6967 - top_1_accuracy: 0.6967 - val_loss: 0.5919 - val_accuracy: 0.6579 - val_top_1_accuracy: 0.6579\n",
      "Epoch 5/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5549 - accuracy: 0.7034 - top_1_accuracy: 0.7034WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 105s 4s/step - loss: 0.5549 - accuracy: 0.7034 - top_1_accuracy: 0.7034 - val_loss: 0.5494 - val_accuracy: 0.7153 - val_top_1_accuracy: 0.7153\n",
      "Epoch 6/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5571 - accuracy: 0.7078 - top_1_accuracy: 0.7078WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 105s 3s/step - loss: 0.5571 - accuracy: 0.7078 - top_1_accuracy: 0.7078 - val_loss: 0.5350 - val_accuracy: 0.7227 - val_top_1_accuracy: 0.7227\n",
      "Epoch 7/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5406 - accuracy: 0.7209 - top_1_accuracy: 0.7209WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 104s 3s/step - loss: 0.5406 - accuracy: 0.7209 - top_1_accuracy: 0.7209 - val_loss: 0.5308 - val_accuracy: 0.7211 - val_top_1_accuracy: 0.7211\n",
      "Epoch 8/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5407 - accuracy: 0.7193 - top_1_accuracy: 0.7193WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 104s 3s/step - loss: 0.5407 - accuracy: 0.7193 - top_1_accuracy: 0.7193 - val_loss: 0.5477 - val_accuracy: 0.7178 - val_top_1_accuracy: 0.7178\n",
      "Epoch 9/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5312 - accuracy: 0.7231 - top_1_accuracy: 0.7231WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 103s 3s/step - loss: 0.5312 - accuracy: 0.7231 - top_1_accuracy: 0.7231 - val_loss: 0.5226 - val_accuracy: 0.7244 - val_top_1_accuracy: 0.7244\n",
      "Epoch 10/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5438 - accuracy: 0.7094 - top_1_accuracy: 0.7094WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 104s 3s/step - loss: 0.5438 - accuracy: 0.7094 - top_1_accuracy: 0.7094 - val_loss: 0.5249 - val_accuracy: 0.7276 - val_top_1_accuracy: 0.7276\n",
      "Epoch 11/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5314 - accuracy: 0.7244 - top_1_accuracy: 0.7244WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 103s 3s/step - loss: 0.5314 - accuracy: 0.7244 - top_1_accuracy: 0.7244 - val_loss: 0.5272 - val_accuracy: 0.7235 - val_top_1_accuracy: 0.7235\n",
      "Epoch 12/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5392 - accuracy: 0.7115 - top_1_accuracy: 0.7115WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 104s 3s/step - loss: 0.5392 - accuracy: 0.7115 - top_1_accuracy: 0.7115 - val_loss: 0.5314 - val_accuracy: 0.7219 - val_top_1_accuracy: 0.7219\n",
      "Epoch 13/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5230 - accuracy: 0.7336 - top_1_accuracy: 0.7336WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 103s 3s/step - loss: 0.5230 - accuracy: 0.7336 - top_1_accuracy: 0.7336 - val_loss: 0.5293 - val_accuracy: 0.7227 - val_top_1_accuracy: 0.7227\n",
      "Epoch 14/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5212 - accuracy: 0.7285 - top_1_accuracy: 0.7285WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 103s 3s/step - loss: 0.5212 - accuracy: 0.7285 - top_1_accuracy: 0.7285 - val_loss: 0.5183 - val_accuracy: 0.7244 - val_top_1_accuracy: 0.7244\n",
      "Epoch 15/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5231 - accuracy: 0.7328 - top_1_accuracy: 0.7328WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 102s 3s/step - loss: 0.5231 - accuracy: 0.7328 - top_1_accuracy: 0.7328 - val_loss: 0.5161 - val_accuracy: 0.7227 - val_top_1_accuracy: 0.7227\n",
      "Epoch 16/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5147 - accuracy: 0.7355 - top_1_accuracy: 0.7355WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 103s 3s/step - loss: 0.5147 - accuracy: 0.7355 - top_1_accuracy: 0.7355 - val_loss: 0.5276 - val_accuracy: 0.7260 - val_top_1_accuracy: 0.7260\n",
      "Epoch 17/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5155 - accuracy: 0.7250 - top_1_accuracy: 0.7250WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 103s 3s/step - loss: 0.5155 - accuracy: 0.7250 - top_1_accuracy: 0.7250 - val_loss: 0.5090 - val_accuracy: 0.7334 - val_top_1_accuracy: 0.7334\n",
      "Epoch 18/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5133 - accuracy: 0.7322 - top_1_accuracy: 0.7322WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 103s 3s/step - loss: 0.5133 - accuracy: 0.7322 - top_1_accuracy: 0.7322 - val_loss: 0.5076 - val_accuracy: 0.7358 - val_top_1_accuracy: 0.7358\n",
      "Epoch 19/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5097 - accuracy: 0.7363 - top_1_accuracy: 0.7363WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 103s 3s/step - loss: 0.5097 - accuracy: 0.7363 - top_1_accuracy: 0.7363 - val_loss: 0.5078 - val_accuracy: 0.7326 - val_top_1_accuracy: 0.7326\n",
      "Epoch 20/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5203 - accuracy: 0.7263 - top_1_accuracy: 0.7263WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 103s 3s/step - loss: 0.5203 - accuracy: 0.7263 - top_1_accuracy: 0.7263 - val_loss: 0.5072 - val_accuracy: 0.7309 - val_top_1_accuracy: 0.7309\n",
      "Epoch 21/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5056 - accuracy: 0.7374 - top_1_accuracy: 0.7374WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 103s 3s/step - loss: 0.5056 - accuracy: 0.7374 - top_1_accuracy: 0.7374 - val_loss: 0.5109 - val_accuracy: 0.7317 - val_top_1_accuracy: 0.7317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5086 - accuracy: 0.7398 - top_1_accuracy: 0.7398WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 103s 3s/step - loss: 0.5086 - accuracy: 0.7398 - top_1_accuracy: 0.7398 - val_loss: 0.5167 - val_accuracy: 0.7276 - val_top_1_accuracy: 0.7276\n",
      "Epoch 23/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5087 - accuracy: 0.7417 - top_1_accuracy: 0.7417WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 104s 3s/step - loss: 0.5087 - accuracy: 0.7417 - top_1_accuracy: 0.7417 - val_loss: 0.5073 - val_accuracy: 0.7334 - val_top_1_accuracy: 0.7334\n",
      "Epoch 24/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5058 - accuracy: 0.7384 - top_1_accuracy: 0.7384WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 104s 3s/step - loss: 0.5058 - accuracy: 0.7384 - top_1_accuracy: 0.7384 - val_loss: 0.5105 - val_accuracy: 0.7301 - val_top_1_accuracy: 0.7301\n",
      "Epoch 25/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5039 - accuracy: 0.7398 - top_1_accuracy: 0.7398WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 104s 3s/step - loss: 0.5039 - accuracy: 0.7398 - top_1_accuracy: 0.7398 - val_loss: 0.5053 - val_accuracy: 0.7252 - val_top_1_accuracy: 0.7252\n",
      "Epoch 26/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5058 - accuracy: 0.7446 - top_1_accuracy: 0.7446WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 105s 3s/step - loss: 0.5058 - accuracy: 0.7446 - top_1_accuracy: 0.7446 - val_loss: 0.5111 - val_accuracy: 0.7334 - val_top_1_accuracy: 0.7334\n",
      "Epoch 27/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5074 - accuracy: 0.7357 - top_1_accuracy: 0.7357WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 107s 4s/step - loss: 0.5074 - accuracy: 0.7357 - top_1_accuracy: 0.7357 - val_loss: 0.5106 - val_accuracy: 0.7309 - val_top_1_accuracy: 0.7309\n",
      "Epoch 28/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5075 - accuracy: 0.7387 - top_1_accuracy: 0.7387WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 106s 4s/step - loss: 0.5075 - accuracy: 0.7387 - top_1_accuracy: 0.7387 - val_loss: 0.5025 - val_accuracy: 0.7276 - val_top_1_accuracy: 0.7276\n",
      "Epoch 29/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5075 - accuracy: 0.7417 - top_1_accuracy: 0.7417WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 106s 4s/step - loss: 0.5075 - accuracy: 0.7417 - top_1_accuracy: 0.7417 - val_loss: 0.5123 - val_accuracy: 0.7383 - val_top_1_accuracy: 0.7383\n",
      "Epoch 30/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5210 - accuracy: 0.7312 - top_1_accuracy: 0.7312WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 107s 4s/step - loss: 0.5210 - accuracy: 0.7312 - top_1_accuracy: 0.7312 - val_loss: 0.5123 - val_accuracy: 0.7334 - val_top_1_accuracy: 0.7334\n",
      "Epoch 31/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5033 - accuracy: 0.7433 - top_1_accuracy: 0.7433WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 107s 4s/step - loss: 0.5033 - accuracy: 0.7433 - top_1_accuracy: 0.7433 - val_loss: 0.5010 - val_accuracy: 0.7276 - val_top_1_accuracy: 0.7276\n",
      "Epoch 32/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5243 - accuracy: 0.7182 - top_1_accuracy: 0.7182WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 108s 4s/step - loss: 0.5243 - accuracy: 0.7182 - top_1_accuracy: 0.7182 - val_loss: 0.5663 - val_accuracy: 0.6743 - val_top_1_accuracy: 0.6743\n",
      "Epoch 33/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5091 - accuracy: 0.7374 - top_1_accuracy: 0.7374WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 108s 4s/step - loss: 0.5091 - accuracy: 0.7374 - top_1_accuracy: 0.7374 - val_loss: 0.5006 - val_accuracy: 0.7334 - val_top_1_accuracy: 0.7334\n",
      "Epoch 34/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5036 - accuracy: 0.7295 - top_1_accuracy: 0.7295WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 108s 4s/step - loss: 0.5036 - accuracy: 0.7295 - top_1_accuracy: 0.7295 - val_loss: 0.5182 - val_accuracy: 0.7227 - val_top_1_accuracy: 0.7227\n",
      "Epoch 35/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5042 - accuracy: 0.7360 - top_1_accuracy: 0.7360WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 107s 4s/step - loss: 0.5042 - accuracy: 0.7360 - top_1_accuracy: 0.7360 - val_loss: 0.6233 - val_accuracy: 0.6464 - val_top_1_accuracy: 0.6464\n",
      "Epoch 36/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5260 - accuracy: 0.7217 - top_1_accuracy: 0.7217WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 107s 4s/step - loss: 0.5260 - accuracy: 0.7217 - top_1_accuracy: 0.7217 - val_loss: 0.5155 - val_accuracy: 0.7358 - val_top_1_accuracy: 0.7358\n",
      "Epoch 37/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5050 - accuracy: 0.7495 - top_1_accuracy: 0.7495WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 107s 4s/step - loss: 0.5050 - accuracy: 0.7495 - top_1_accuracy: 0.7495 - val_loss: 0.5081 - val_accuracy: 0.7293 - val_top_1_accuracy: 0.7293\n",
      "Epoch 38/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.4999 - accuracy: 0.7422 - top_1_accuracy: 0.7422WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 107s 4s/step - loss: 0.4999 - accuracy: 0.7422 - top_1_accuracy: 0.7422 - val_loss: 0.5050 - val_accuracy: 0.7383 - val_top_1_accuracy: 0.7383\n",
      "Epoch 39/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.4939 - accuracy: 0.7473 - top_1_accuracy: 0.7473WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 107s 4s/step - loss: 0.4939 - accuracy: 0.7473 - top_1_accuracy: 0.7473 - val_loss: 0.5012 - val_accuracy: 0.7276 - val_top_1_accuracy: 0.7276\n",
      "Epoch 40/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5063 - accuracy: 0.7333 - top_1_accuracy: 0.7333WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 107s 4s/step - loss: 0.5063 - accuracy: 0.7333 - top_1_accuracy: 0.7333 - val_loss: 0.5350 - val_accuracy: 0.7227 - val_top_1_accuracy: 0.7227\n",
      "Epoch 41/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.5043 - accuracy: 0.7365 - top_1_accuracy: 0.7365WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 107s 4s/step - loss: 0.5043 - accuracy: 0.7365 - top_1_accuracy: 0.7365 - val_loss: 0.4993 - val_accuracy: 0.7383 - val_top_1_accuracy: 0.7383\n",
      "Epoch 42/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.4916 - accuracy: 0.7465 - top_1_accuracy: 0.7465WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 107s 4s/step - loss: 0.4916 - accuracy: 0.7465 - top_1_accuracy: 0.7465 - val_loss: 0.4999 - val_accuracy: 0.7391 - val_top_1_accuracy: 0.7391\n",
      "Epoch 43/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.4998 - accuracy: 0.7403 - top_1_accuracy: 0.7403WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/29 [===============================] - 107s 4s/step - loss: 0.4998 - accuracy: 0.7403 - top_1_accuracy: 0.7403 - val_loss: 0.5036 - val_accuracy: 0.7408 - val_top_1_accuracy: 0.7408\n",
      "Epoch 44/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.4939 - accuracy: 0.7497 - top_1_accuracy: 0.7497WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 107s 4s/step - loss: 0.4939 - accuracy: 0.7497 - top_1_accuracy: 0.7497 - val_loss: 0.5143 - val_accuracy: 0.7375 - val_top_1_accuracy: 0.7375\n",
      "Epoch 45/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.4939 - accuracy: 0.7425 - top_1_accuracy: 0.7425WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 107s 4s/step - loss: 0.4939 - accuracy: 0.7425 - top_1_accuracy: 0.7425 - val_loss: 0.5013 - val_accuracy: 0.7391 - val_top_1_accuracy: 0.7391\n",
      "Epoch 46/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.4890 - accuracy: 0.7527 - top_1_accuracy: 0.7527WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 106s 4s/step - loss: 0.4890 - accuracy: 0.7527 - top_1_accuracy: 0.7527 - val_loss: 0.4972 - val_accuracy: 0.7358 - val_top_1_accuracy: 0.7358\n",
      "Epoch 47/50\n",
      "30/29 [===============================] - ETA: -2s - loss: 0.4869 - accuracy: 0.7554 - top_1_accuracy: 0.7554WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 114s 4s/step - loss: 0.4869 - accuracy: 0.7554 - top_1_accuracy: 0.7554 - val_loss: 0.5155 - val_accuracy: 0.7367 - val_top_1_accuracy: 0.7367\n",
      "Epoch 48/50\n",
      "30/29 [===============================] - ETA: -4s - loss: 0.4918 - accuracy: 0.7500 - top_1_accuracy: 0.7500WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 180s 6s/step - loss: 0.4918 - accuracy: 0.7500 - top_1_accuracy: 0.7500 - val_loss: 0.4993 - val_accuracy: 0.7416 - val_top_1_accuracy: 0.7416\n",
      "Epoch 49/50\n",
      "30/29 [===============================] - ETA: -4s - loss: 0.4910 - accuracy: 0.7468 - top_1_accuracy: 0.7468WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 195s 7s/step - loss: 0.4910 - accuracy: 0.7468 - top_1_accuracy: 0.7468 - val_loss: 0.4965 - val_accuracy: 0.7424 - val_top_1_accuracy: 0.7424\n",
      "Epoch 50/50\n",
      "30/29 [===============================] - ETA: -4s - loss: 0.5104 - accuracy: 0.7379 - top_1_accuracy: 0.7379WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "30/29 [===============================] - 195s 7s/step - loss: 0.5104 - accuracy: 0.7379 - top_1_accuracy: 0.7379 - val_loss: 0.5012 - val_accuracy: 0.7424 - val_top_1_accuracy: 0.7424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb8ac38d290>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from time import time\n",
    "from plot_keras_history import plot_history\n",
    "\n",
    "#tensorboard = TensorBoard(log_dir=\"logs/layers_frozen_{}\".format(num_layers_to_freeze))\n",
    "tensorboard = TensorBoard(log_dir=\"logs/layers_frozen\")\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"osf_vggish_weights_train_last_layers.best.hdf5\"\n",
    "best_model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [best_model_checkpoint, tensorboard]\n",
    "\n",
    "# parallel_model.fit_generator(\n",
    "#     training_generator,\n",
    "#     steps_per_epoch=nb_training_samples/batch_size,\n",
    "#     epochs=epochs,\n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=nb_validation_samples/batch_size,\n",
    "#     callbacks=callbacks_list)\n",
    "\n",
    "model.fit_generator(\n",
    "    training_generator,\n",
    "    steps_per_epoch=nb_training_samples/batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples/batch_size,\n",
    "    callbacks=callbacks_list)\n",
    "# parallel_model.fit_generator(\n",
    "#     training_generator,\n",
    "#     samples_per_epoch=nb_training_samples,\n",
    "#     epochs=epochs,\n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=nb_validation_samples/batch_size,)\n",
    "#     nb_val_samples=nb_validation_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get top k predictions for selected test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_predictions(preds, label_map, k=1, print_flag=False):\n",
    "    sorted_array = np.argsort(preds)[::-1]\n",
    "    top_k = sorted_array[:k]\n",
    "    label_map_flip = dict((v,k) for k,v in label_map.items())\n",
    "    \n",
    "    y_pred = []\n",
    "    for label_index in top_k:\n",
    "        if print_flag:\n",
    "            print (\"{} ({})\".format(label_map_flip[label_index], preds[label_index]))\n",
    "        y_pred.append(label_map_flip[label_index])\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sick']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = (training_generator.class_indices)\n",
    " \n",
    "json2 = json.dumps(label_map)\n",
    "f = open(\"cough_label_map.json\",\"w\")\n",
    "f.write(json2)\n",
    "f.close()\n",
    "\n",
    "img_path = '/Users/xt/Desktop/OSF/melspectrograms/test/sick/audioset__3RvCwwIZ4w_10_15.png'\n",
    "img = image.load_img(img_path, target_size=(128,128), color_mode=\"grayscale\")\n",
    "#x = image.random_rotation(image.img_to_array(img), rg = 90)\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "x = np.expand_dims(x, axis=0)* 1./255\n",
    "\n",
    "preds = model.predict(x)[0]\n",
    "\n",
    "get_top_k_predictions(preds, label_map, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128 at 0x7FB6AAEE4C90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate and plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "    plt.figure(figsize=(24,24))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x1728 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import requests\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "from keras.preprocessing import image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "testing_dir = '/Users/xt/Desktop/OSF/melspectrograms/test/'\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for label in label_map.keys():\n",
    "    file_list = os.listdir(testing_dir + label)\n",
    "    for file_name in file_list:\n",
    "        if file_name != '.DS_Store':\n",
    "            img_path = testing_dir + label + '/' + file_name\n",
    "            img = image.load_img(img_path, target_size=(128,128), color_mode=\"grayscale\")\n",
    "            \n",
    "            x = image.img_to_array(img)\n",
    "            #x = image.random_rotation(image.img_to_array(img), rg = 90)\n",
    "            x = np.expand_dims(x, axis=0)* 1./255\n",
    "    \n",
    "            preds = model.predict(x)[0]\n",
    "            \n",
    "            y_true.append(label)\n",
    "            y_pred.append(get_top_k_predictions(preds, label_map, k=1)[0])\n",
    "        \n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cm, sorted(label_map.keys()), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
